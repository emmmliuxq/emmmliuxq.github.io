{"pages":[{"title":"404","text":"","path":"404/index.html","date":"06-04","excerpt":""},{"title":"about","text":"","path":"about/index.html","date":"06-04","excerpt":""},{"title":"categories","text":"type: “categirues”layout: “categories”","path":"categories/index.html","date":"06-04","excerpt":""},{"title":"message","text":"type: “message”layout: “message”","path":"message/index.html","date":"06-04","excerpt":""},{"title":"search","text":"","path":"search/index.html","date":"06-04","excerpt":""},{"title":"tags","text":"type: “tags”layout: “tags”","path":"tags/index.html","date":"06-04","excerpt":""}],"posts":[{"title":"ubuntu16.04安装hadoop3.02(伪分布式)+集群","text":"##单机模式 环境已经配好ubuntu16.04安装hadoop3.02单机模式 ##配置Hadoop伪分布式环境1cd /usr/local/hadoop/etc/hadoop/ ###修改core-site.xml1234567891011&lt;configuration&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;file:/usr/local/hadoop/tmp&lt;/value&gt; &lt;description&gt;Abase for other temporary directories.&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://localhost:9000&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; ###修改hdfs-site.xml1234567891011121314&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:/usr/local/hadoop/tmp/dfs/name&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;file:/usr/local/hadoop/tmp/dfs/data&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 进行NameNode格式化进入/usr/local/hadoop ###执行./bin/hdfs namenode -format (第一次设置的时候格式化 下次直接启动就好)启动NameNode 和 DataNode1./sbin/start-dfs.sh 输入jps，会出现如下进程 ##访问localhost:9870(hadoop3.X的webUI已经改到端口 localhost:9870上面，而不是原来的50070，网上大多数的教程都是2.X的,当然不行)关闭dfs1./sbin/stop-dfs.sh ##配置Yarn1cd /usr/local/hadoop/etc/hadoop ###1 配置mapred-site.xml (hadoop2.x 没有 mapred-site.xml 需要自己创建 hadoop3.x 可以直接打开 mapred-site.xml）配置mapred-site.xml mv mapred-site.xml.template mapred-site.xml #hadoop2.x 执行此命令 hadoop3.x 不必执行 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; ###2 配置yarn-site.xml123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; ##3 启动Yarn cd /usr/local/hadoop/ 123./sbin/start-dfs.sh #启动dfs./sbin/start-yarn.sh./sbin/mr-jobhistory-daemon.sh start historyserver ###前面我已经启动过dfs了 所以。。 ##访问localhost:8088 ###关闭Yarn 123./sbin/stop-dfs.sh./sbin/stop-yarn.sh./sbin/mr-jobhistory-daemon.sh stop historyserver ###集群 ####说明 :集群是在伪分布式是的基础上搭建的 只需要改一些配置文件 然后 把配置好的master克隆就行 1 修改 /etc/hostnamesudo vim /etc/hostname将内容修改为master/slave1/slave2 2 修改 /etc/hosts 3 配置 master 节点可通过 SSH 无密码访问 slave1####单机模式 已经配过ssh免密码登录 所以把 master 的 authorized_keys拷贝到slave1 的 ./ssh文件下即可ubuntu 16.04安装hadoop3.02单机模式 ###master 尝试无密码登录slave1的ssh ##修改配置文件 1 修改 core-site.xmlcd /usr/local/hadoop/etc/hadoop 1234567891011&lt;configuration&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;file:/usr/local/hadoop/tmp&lt;/value&gt; &lt;description&gt;Abase for other temporary directories.&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://master:9000&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 解释下：第一个fs.defaultFS设置master机为namenode hadoop.tmp.dir配置Hadoop的一个临时目录，用来存放每次运行的作业jpb的信息。 2 修改 hdfs-site.xml123456789101112131415161718&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;2&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:/usr/local/hadoop/tmp/dfs/name&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;file:/usr/local/hadoop/tmp/dfs/data&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;slave1:9001&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; dfs.namenode.name.dir是namenode存储永久性的元数据的目录列表。这个目录会创建在master机上。dfs.namenode.data.dir是datanode存放数据块的目录列表，这个目录在slave11和slave1机都会创建。 dfs.replication 设置文件副本数，这里两个datanode，所以设置副本数为2。 配置mapred-site.xml1234567891011121314151617181920&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.application.classpath&lt;/name&gt; &lt;value&gt; /usr/local/hadoop/etc/hadoop, /usr/local/hadoop/share/hadoop/common/*, /usr/local/hadoop/share/hadoop/common/lib/*, /usr/local/hadoop/share/hadoop/hdfs/*, /usr/local/hadoop/share/hadoop/hdfs/lib/*, /usr/local/hadoop/share/hadoop/mapreduce/*, /usr/local/hadoop/share/hadoop/mapreduce/lib/*, /usr/local/hadoop/share/hadoop/yarn/*, /usr/local/hadoop/share/hadoop/yarn/lib/* &lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 解释下：这里设置的是运行jobtracker的服务器主机名和端口，也就是作业将在master主机的9001端口执行 配置yarn-site.xml12345678910111213141516171819202122&lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.resource-tracker.address&lt;/name&gt; &lt;value&gt;master:8025&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.scheduler.address&lt;/name&gt; &lt;value&gt;master:8030&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.address&lt;/name&gt; &lt;value&gt;master:8040&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; ###从master机器启动hadoop集群服务 进入/usr/local/hadoop #####执行./bin/hdfs namenode -format 进行格式化操作 ####启动 ./sbin/start-all.sh ###slave1 jps查看 ####浏览器打开master:9870 ####浏览器打开master:8088 ###master关闭集群服务 ###参考Hadoop3.0.0在Ubuntu16.04上分布式部署ubuntu16.04搭建hadoop集群环境hadoop分布式集群安装","path":"2018/11/11/uhadoop3(2)/","date":"11-11","excerpt":""},{"title":"ubuntu16.04安装hadoop3.02(单机模式)","text":"##1创建hadoop 用户组 sudo addgroup hadoop ##创建Hadoop用户 sudo adduser -ingroup hadoop hadoop ###回车后会提示输入密码(hadoop用户的密码) 然后一直敲回车就好 ##为Hadoop用户添加权限 sudo gedit /etc/sudoers 打开sudoers文件，找到root ALL=(ALL:ALL) ALL修改后如图 ##切换到hadoop用户 su - hadoop 输入密码即可 ###安装ssh sudo apt-get install openssh-server 启动ssh服务： sudo /etc/init.d/ssh start 启动后，可以通过如下命令来确认服务是否正确启动： ps -e | grep ss 作为一个安全通信协议，使用时需要密码，因此我们要设置成免密码登录，生成私钥和公钥：1ssh-keygen -t rsa -P &quot;&quot; 此时会在／home／hadoop/.ssh下生成两个文件：id_rsa和id_rsa.pub，前者为私钥，后者为公钥。现在我们将公钥追加到authorized_keys中（authorized_keys用于保存所有允许以当前用户身份登录到ssh客户端用户的公钥内容）： cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys 现在可以登入ssh确认以后登录时不用输入密码 ssh localhost 退出 exit ##安装Java： sudo apt-get install openjdk-6-jdk 直接执行此命令 会报错(因为Ubuntu16.04的安装源已经默认没有openjdk7了，所以要自己手动添加仓库)在网上找到的解决办法123sudo add-apt-repository ppa:openjdk-r/ppa sudo apt-get update sudo apt-get install openjdk-7-jdk 但是速度是真的慢(考虑到是国外的库)所以安装失败另一种 Ubuntu 16.04 Java8 安装这种我没尝试 ###最终我采用的解决办法是在Ubuntu 16.04如何安装Java ###安装默认JRE / JDK安装Java的最简单的选择是使用与Ubuntu一起打包的版本。 具体来说，这将安装OpenJDK 8，最新和推荐的版本。 首先，更新包索引。 sudo apt-get update 接下来，安装Java。 具体来说，此命令将安装Java运行时环境（JRE）。1sudo apt-get install default-jre 还有另一个默认的Java安装，称为JDK（Java开发工具包）。 JDK通常只需要如果你要编译Java程序，或者如果使用Java的软件特别需要它。 ###JDK确实包含JRE，因此如果安装JDK而不是JRE，除了较大的文件大小，则没有缺点。 您可以使用以下命令安装JDK：1sudo apt-get install default-jdk 查看java 版本信息 java -version ##安装Hadoop根据自己需要下载下载地址首先是切换到hadoop用户要确保所有的操作都是在用户hadoop下完成的 ###解压 tar zxf hadoop-3.0.2.tar.gz ###移动(将解压后的文件 移动到 /usr/local/hadoop目录下) sudo mv hadoop-3.0.2 /usr/local/hadoop ###赋权 sudo chown -R hadoop:hadoop /usr/local/hadoop ##配置Hadoop ###1配置.bashrc配置该文件，需要知道Java的安装路径 update-alternatives --config java ###修改.bashrc 文件 sudo gedit ~/.bashrc 在文件末尾追加下面内容，然后保存。123456789101112#HADOOP VARIABLES STARTexport JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64export HADOOP_INSTALL=/usr/local/hadoopexport PATH=$PATH:$HADOOP_INSTALL/binexport PATH=$PATH:$HADOOP_INSTALL/sbinexport HADOOP_MAPRED_HOME=$HADOOP_INSTALLexport HADOOP_COMMON_HOME=$HADOOP_INSTALLexport HADOOP_HDFS_HOME=$HADOOP_INSTALLexport YARN_HOME=$HADOOP_INSTALLexport HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_INSTALL/lib/nativeexport HADOOP_OPTS=&quot;-Djava.library.path=$HADOOP_INSTALL/lib&quot;#HADOOP VARIABLES END 执行下面命令，使添加的环境变量生效： source ~/.bashrc ###配置hadoop-env.sh sudo gedit /usr/local/hadoop/etc/hadoop/hadoop-env.sh 找到JAVA_HOME变量，按如下进行修改 export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64 使配置生效： source /usr/local/hadoop/etc/hadoop/hadoop-env.sh ####单机模式安装完成 输入 hadoop version查看版本信息 ##ubuntu16.04安装hadoop3.02伪分布式ubuntu16.04安装hadoop3.02伪分布式+集群 ###参考文章:# 在Ubuntu系统上一步步搭建Hadoop（单机模式） Ubuntu16.04 安装hadoop伪分布式1","path":"2018/11/11/uhadoop3(1)/","date":"11-11","excerpt":""},{"title":"Dvwa-xss(DOM)","text":"##XSS 简介XSS（Cross Site Script），全称跨站脚本攻击，为了与 CSS（Cascading Style Sheet） 有所区别，所以在安全领域称为 XSS。XSS 攻击，通常指黑客通过 HTML 注入 篡改网页，插入恶意脚本，从而在用户浏览网页时，控制用户浏览器的一种攻击行为。 #####DOM—based XSS漏洞的产生DOM—based XSS漏洞是基于文档对象模型Document Objeet Model，DOM)的一种漏洞。DOM是一个与平台、编程语言无关的接口，它允许程序或脚本动态地访问和更新文档内容、结构和样式，处理后的结果能够成为显示页面的一部分。DOM中有很多对象，其中一些是用户可以操纵的，如uRI，location，refelTer等。客户端的脚本程序可以通过DOM动态地检查和修改页面内容，它不依赖于提交数据到服务器端，而从客户端获得DOM中的数据在本地执行，如果DOM中的数据没有经过严格确认，就会产生DOM—based XSS漏洞。DOM简介xss DOM可能触发DOM型XSS的属性： document.referer属性 window.name属性 location属性 innerHTML属性 documen.write属性 ###1 查看服务端代码什么也没有 ####查看页面源代码 ###相关属性方法解释 ####1 document 和 windows 对象document表示的是一个文档对象，window表示的是一个窗口对象，一个窗口下可以有多个文档对象。 所以一个窗口下只有一个window.location.href，但是可能有多个document.URL、document.location.href window 对象 它是一个顶层对象,而不是另一个对象的属性即浏览器的窗口。 document 对象该对象是window和frames对象的一个属性,是显示于窗口或框架内的一个文档。 document 只是属于window 的一个子对像。 window.location 包含 href 属性，直接取值赋值时相当于 window.location.href window.location.href 当前页面完整 URL document.location 包含 href 属性，直接取值赋值时相当于 document.location.href document.location.href 当前页面完整 URL document.href 没有这个属性 document.URL 取值时等价于 window.location.href 或 document.location.href。在某些浏览器中通过对 document.URL 赋值来实现页面跳转，但某些浏览器中不行。 ####2 indexOf()方法定义和用法indexOf() 方法可返回某个指定的字符串值在字符串中首次出现的位置。 ####indexOf(searchvalue,fromindex) 参数 描述 searchvalue 必需 规定需检索的字符串值。 fromindex 可选的整数参数 规定在字符串中开始检索的位置 它的合法取值是 0 到 stringObject.length - 1 fromindex 如省略该参数，则将从字符串的首字符开始检索。 注释：indexOf() 方法对大小写敏感！注释：如果要检索的字符串值没有出现，则该方法返回 -1 ####2 substring() 方法substring() 方法用于提取字符串中介于两个指定下标之间的字符 ####substring(start,stop) 参数 描述 start 必需。 一个非负的整数，规定要提取的子串的第一个字符在 stringObject 中的位置。 stop 可选。 一个非负的整数，比要提取的子串的最后一个字符在 stringObject 中的位置多 1。 stop 如果省略该参数，那么返回的子串会一直到字符串的结尾。 ######decodeURI() 函数可对 encodeURI() 函数编码过的 URI 进行解码 document.writedocument.write详解document.write是JavaScript中对document.open所开启的文档流(document stream操作的API方法，它能够直接在文档流中写入字符串，一旦文档流已经关闭，那document.write就会重新利用document.open打开新的文档流并写入，此时原来的文档流会被清空，已渲染好的页面就会被清除，浏览器将重新构建DOM并渲染新的页面 #####解释script 代码123456 if (document.location.href.indexOf(&quot;default=&quot;) &gt;= 0) #判断 &quot;default=&quot; 是否存在 var lang = document.location.href.substring(document.location.href.indexOf(&quot;default=&quot;)+8) # 取出 default 的值 并 赋值给变量langdocument.write(&quot;&lt;option value=&apos;&quot; + lang + &quot;&apos;&gt;&quot; + decodeURI(lang) + &quot;&lt;/option&gt;&quot;);写入&lt;option value=&apos;&quot;lang&quot;&apos;&gt;&quot;decodeURL(lang)&lt;/option&gt; 所以我们插入的 javascript 代码可以在 decodeURL(lang) 被执行 ###构造攻击语句1http://127.0.0.1/dvwa-master/vulnerabilities/xss_d/?default=English&lt;script&gt;alert(/xss/);&lt;/script&gt; ####写入页面的效果是这样的1&lt;option value=&apos;变量lang 的值&apos;&gt;English&lt;script&gt;alert(/xss/);&lt;/script&gt;&lt;/option&gt; ##Medium ####查看服务端源代码 array_key_exists() 函数检查某个数组中是否存在指定的键名，如果键名存在则返回 true，如果键名不存在则返回 false。提示：如果指定数组的时候省略了键名，将会生成从 0 开始并以 1 递增的整数键名 ####array_key_exists(key,array) 参数 描述 key 必需 规定键名。 array 必需 规定数组 stripos() 函数查找字符串在另一字符串中第一次出现的位置（不区分大小写） ####stripos(string,find,start) 参数 描述 string 必需 规定被搜索的字符串。 find 必需 规定要查找的字符。 start 可选 规定开始搜索的位置 返回值 返回字符串在另一字符串中第一次出现的位置，如果没有找到字符串则返回 FALSE 返回值 注释：字符串位置从 0 开始，不是从 1 开始。 header() 函数向客户端发送原始的 HTTP 报头 ####header(string,replace,http_response_code) 参数 描述 string 必需 规定要发送的报头字符串。 replace 可选 指示该报头是否替换之前的报头，或添加第二个报头。 replace 默认是 true（替换）。false（允许相同类型的多个报头）。 http_response_code可选 把 HTTP 响应代码强制为指定的值。（PHP 4 以及更高版本可用） ####由服务端代码 可知 Medium 级别过滤了 ####构造攻击语句1http://127.0.0.1/dvwa-master/vulnerabilities/xss_d/?default=English&lt;/option&gt;&lt;/select&gt;&lt;img src=1 onerror=alert(/xss/)&gt; ####写入页面的效果是这样的1&lt;option value=&apos;&apos;&gt; English&lt;/option&gt;&lt;/select&gt;&lt;img src=1 onerror=alert(/xss/)&gt;&lt;/option&gt; 首先闭合了标签 和 标签利用 img标签的onerror事件javascript，img标签支持onerror 事件，在加载图像的过程中如果发生了错误，就会触发onerror事件执行 JavaScript ###high ####查看服务端源代码 ###白名单 只允许 传的 default值 为 French English German Spanish 其中一个 ####构造攻击语句1http://www.dvwa.com/vulnerabilities/xss_d/?default=English #&lt;script&gt;alert(/xss/)&lt;/script&gt; ####写入页面的效果是这样的1&lt;option value=&apos;&apos;&gt;English #&lt;script&gt;alert(/xss/)&lt;/script&gt;&lt;/option&gt; 由于 form表单提交的数据 想经过JS 过滤 所以注释部分的javascript 代码 不会被传到服务器端(也就符合了白名单的要求) #####我们写一个html 验证一下English #alert(/xss/)是否可行 ##impossible ####服务端源代码不需要做任何事，在客户端处理 #####网页源代码变了","path":"2018/11/11/dvwa_xss(dom)/","date":"11-11","excerpt":""},{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","path":"2018/06/04/hello-world/","date":"06-04","excerpt":""}]}